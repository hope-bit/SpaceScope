import h5py
import numpy as np
import os
from datafed.CommandLib import API
'''
# Initialize the DataFed API
df_api = API()

# Dataset IDs in DataFed
train_dataset_id = "d/525641772"
test_dataset_id = "d/525641748"

def download_dataset(dataset_id, output_dir="data"):
    import time
    
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)
    print(f"Attempting to download dataset {dataset_id} to {output_dir}")
    
    # Trigger the download request using the DataFed API (df_api)
    response = df_api.dataGet(dataset_id, output_dir)
    print("DataGet result:", response)

    # Extract the task ID from the response (task ID will be used to track download status)
    task_id = None
    for line in str(response).split("\n"):
        if "task/" in line:
            task_id = line.split('"')[1]  # Extract task ID from response
            break

    if not task_id:
        raise RuntimeError("Failed to retrieve task ID from DataGet response.")
    
    print(f"Task initiated with ID: {task_id}")

    # Wait for the task to complete (polling every 5 seconds)
    while True:
        try:
            task_details = df_api.taskView(task_id)  # Fetch task details using the task ID
        except Exception as e:
            raise RuntimeError(f"Failed to fetch task status: {e}")
        
        print(f"Task details: {task_details}")
        
        # Handle the task details, assuming it's a tuple or dictionary-like object
        if isinstance(task_details, tuple):
            print("Task details is a tuple. Inspecting first element:", task_details[0])
            task_status = task_details[0]  # Get the first element if it's a tuple
        else:
            print("Task details structure:", task_details)
            task_status = task_details  # If it's not a tuple, use the entire response
        
        print(f"Current task status: {task_status}")
        
        # Extract the task status (assuming it's a dictionary or direct status string)
        if isinstance(task_status, dict):
            status = task_status.get('status', None)
        else:
            status = task_status  # Direct assignment if task status is not a dictionary
        
        # Check the task status and act accordingly
        if status == "TS_COMPLETE":
            print("Task completed successfully.")
            break
        elif status in ["TS_ERROR", "TS_ABORTED"]:
            raise RuntimeError(f"DataGet task failed with status: {status}")
        else:
            print("Task still in progress. Waiting...")
            time.sleep(30)  # Wait for 5 seconds before checking the status again

    # Check the output directory for the downloaded dataset
    files = os.listdir(output_dir)
    print("Files in output_dir:", files)
    
    # Find the downloaded dataset based on the dataset_id
    for file_name in files:
        if dataset_id.split('/')[1] in file_name:  # Match the dataset ID in the filename
            dataset_path = os.path.join(output_dir, file_name)
            print(f"Dataset {dataset_id} found at {dataset_path}")
            return dataset_path

    # If dataset is not found, raise an error
    raise FileNotFoundError(f"Dataset {dataset_id} could not be found in {output_dir}")


# Paths to downloaded datasets
train_file_path = download_dataset(train_dataset_id)
test_file_path = download_dataset(test_dataset_id)

'''
# Paths to local datasets
train_file_path = 'C:/Users/HopeW/Downloads/MEM679_Project/SpaceScope/galaxy_mnist/GalaxyMNIST/raw/train_dataset.hdf5'
test_file_path = 'C:/Users/HopeW/Downloads/MEM679_Project/SpaceScope/galaxy_mnist/GalaxyMNIST/raw/test_dataset.hdf5'

# Load training dataset
with h5py.File(train_file_path, 'r') as train_file:
    X_train = train_file['images'][:] / 255.0  # Normalize to [0, 1]
    y_train = train_file['labels'][:]

# Load test dataset
with h5py.File(test_file_path, 'r') as test_file:
    X_test = test_file['images'][:] / 255.0  # Normalize to [0, 1]
    y_test = test_file['labels'][:]

y_train = np.squeeze(y_train)
y_test = np.squeeze(y_test)


import tensorflow as tf
import keras
from keras import Sequential, layers, models
from keras._tf_keras.keras.utils import to_categorical

# Convert labels to one-hot encoding
#from sklearn.preprocessing import OneHotEncoder
print("y_train shape:", y_train.shape)
print("y_train values:", y_train[:10])  # Print first 10 values to inspect
print("y_test shape:", y_test.shape)
print("y_test values:", y_test[:10])

# One-hot encode labels
y_train_one_hot = to_categorical(y_train, num_classes=4)
y_test_one_hot = to_categorical(y_test, num_classes=4)

print("y_train_one_hot shape:", y_train_one_hot.shape)
print("y_test_one_hot shape:", y_test_one_hot.shape)
y_train_one_hot = y_train_one_hot.astype('float32')
y_test_one_hot = y_test_one_hot.astype('float32')


# Define the model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
    layers.Conv2D(256, (3, 3), activation='relu'),
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
    layers.Flatten(),
    
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.7),  # Add dropout to prevent overfitting
    
    layers.Dense(4, activation='softmax')  # Output layer with 4 classes
])

# Compile the model
from keras._tf_keras.keras.optimizers import RMSprop

model.compile(optimizer=RMSprop(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Print model summary
model.summary()
print(model.output_shape)

from keras._tf_keras.keras.preprocessing.image import ImageDataGenerator
from keras._tf_keras.keras.callbacks import ReduceLROnPlateau, EarlyStopping
import numpy as np

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

datagen.fit(X_train)

# Reduce learning rate on plateau
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model with augmented data and callbacks
history = model.fit(
    datagen.flow(X_train, y_train_one_hot, batch_size=32),
    epochs=20,
    validation_data=(X_test, y_test_one_hot),
    callbacks=[lr_scheduler, early_stopping]
)

# Evaluate on test dataset
test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)
print(f"Test Accuracy: {test_accuracy:.2f}")

import matplotlib.pyplot as plt

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.show()

# Save the model
model.save('galaxy_mnist_classifierH5.h5')
model.save('galaxy_mnist_classifierK.keras')

# Load and predict
loaded_model = tf.keras.models.load_model('galaxy_mnist_classifierK.keras')
predictions = loaded_model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
